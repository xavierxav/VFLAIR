{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from dataset.party_dataset import ActiveSatelliteDataset, SatteliteLABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 168, 168])\n"
     ]
    }
   ],
   "source": [
    "data_root = \"C:/Users/XD278777/Desktop/worldstrat/dataset\"\n",
    "\n",
    "data = pd.read_csv(data_root + r'\\metadata.csv')\n",
    "# Rename 1st column to 'POI'\n",
    "data = data.rename(columns={data.columns[0]: 'POI'})\n",
    "data = data.loc[data['POI'] != 'ASMSpotter-1-1-1']\n",
    "POI = data['POI'].unique()\n",
    "dataset = ActiveSatelliteDataset(test_train_POI=POI, index=None)\n",
    "print(dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the feature extractor and the model\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "\n",
    "image = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Preprocess the image\n",
    "# inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    features = model(image).last_hidden_state\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(224/16)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XD278777\\AppData\\Local\\miniconda3\\envs\\VFLAIR\\lib\\site-packages\\torch\\nn\\functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# print(image.shape)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "print(last_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel\n",
    "import tifffile as tiff\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_path = r'C:\\Users\\XD278777\\Desktop\\worldstrat\\dataset\\lr_dataset\\Amnesty POI-1-1-1\\L2A\\Amnesty POI-1-1-1-2-L2A_data.tiff'\n",
    "image = tiff.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 168, 168])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XD278777\\AppData\\Local\\miniconda3\\envs\\VFLAIR\\lib\\site-packages\\torch\\nn\\functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image)[1].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/123 [00:00<?, ?it/s]c:\\Users\\XD278777\\AppData\\Local\\miniconda3\\envs\\VFLAIR\\lib\\site-packages\\torch\\nn\\functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "Processing images: 100%|██████████| 123/123 [13:43<00:00,  6.69s/it]\n",
      "Saving class tokens: 100%|██████████| 123/123 [00:00<00:00, 159.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class token extraction completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel\n",
    "import tifffile as tiff\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Define a function to extract the class token from a batch of images\n",
    "def extract_class_token_batch(image_paths, model, transform):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        image = tiff.imread(image_path)\n",
    "        image = transform(image)\n",
    "        images.append(image)\n",
    "    images = torch.stack(images).to(device)  # Stack and move to device (GPU)\n",
    "    with torch.no_grad():\n",
    "        features = model(images)\n",
    "    class_tokens = features[1].squeeze().cpu().numpy()  # Assuming the class token is the first token\n",
    "    return class_tokens\n",
    "\n",
    "# Directory paths\n",
    "input_dir = r'C:\\Users\\XD278777\\Desktop\\worldstrat\\dataset\\lr_dataset'  # Change to your input directory\n",
    "output_dir = r'C:\\Users\\XD278777\\Desktop\\worldstrat\\dataset\\extracted_features'  # Change to your desired output directory\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load DINOv2 model\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "model.eval()\n",
    "\n",
    "# Transform definition\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda image: torch.tensor(image.reshape(12, 168, 168)[2:5]))  # Reshape and select channels 2, 3, 4\n",
    "])\n",
    "\n",
    "# Collect all files\n",
    "files_to_process = []\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.tiff'):\n",
    "            files_to_process.append(os.path.join(root, file))\n",
    "\n",
    "# Batch size for processing images\n",
    "batch_size = 512\n",
    "\n",
    "# Function to save class tokens\n",
    "def save_class_tokens(batch_file_paths, class_tokens, output_dir):\n",
    "    for file_path, class_token in zip(batch_file_paths, class_tokens):\n",
    "        relative_path = os.path.relpath(os.path.dirname(file_path), input_dir)\n",
    "        output_path = os.path.join(output_dir, relative_path)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        class_token_file_path = os.path.join(output_path, os.path.basename(file_path).replace('.tiff', '_class_token.npy'))\n",
    "        np.save(class_token_file_path, class_token)\n",
    "\n",
    "# Process files in batches with progress bar\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in tqdm(range(0, len(files_to_process), batch_size), desc=\"Processing images\"):\n",
    "        batch_file_paths = files_to_process[i:i+batch_size]\n",
    "        class_tokens = extract_class_token_batch(batch_file_paths, model, transform)\n",
    "        futures.append(executor.submit(save_class_tokens, batch_file_paths, class_tokens, output_dir))\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for future in tqdm(futures, desc=\"Saving class tokens\"):\n",
    "        future.result()\n",
    "\n",
    "print(\"Class token extraction completed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VFLAIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
